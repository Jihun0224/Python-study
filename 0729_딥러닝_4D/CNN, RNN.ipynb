{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "962741e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a61091c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7009880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPool2D\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bc573ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7842855",
   "metadata": {},
   "source": [
    "# Wequential 클래스 활용, 1번 히든 레이어 16개 노드 구성, 출력층에 클래스에 맞게 노드 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d98386e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d942bc07",
   "metadata": {},
   "source": [
    "### softmax  \n",
    "- 세 개 이상으로 분류하는 다중 클래스 분류에서 사용되는 활성화 함수  \n",
    "- 분류될 클래스가 n개라 할 때, n차원의 벡터를 입력받아, 각 클래스에 속할 확률을 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89a8fbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "150/150 [==============================] - 1s 870us/step - loss: 1.3756 - accuracy: 0.2800\n",
      "Epoch 2/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.8878 - accuracy: 0.5933\n",
      "Epoch 3/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.7264 - accuracy: 0.7267\n",
      "Epoch 4/100\n",
      "150/150 [==============================] - 0s 877us/step - loss: 0.6287 - accuracy: 0.7667\n",
      "Epoch 5/100\n",
      "150/150 [==============================] - 0s 850us/step - loss: 0.5664 - accuracy: 0.7600\n",
      "Epoch 6/100\n",
      "150/150 [==============================] - 0s 850us/step - loss: 0.5167 - accuracy: 0.8400\n",
      "Epoch 7/100\n",
      "150/150 [==============================] - 0s 837us/step - loss: 0.4776 - accuracy: 0.8867\n",
      "Epoch 8/100\n",
      "150/150 [==============================] - 0s 930us/step - loss: 0.4449 - accuracy: 0.9200\n",
      "Epoch 9/100\n",
      "150/150 [==============================] - 0s 930us/step - loss: 0.4157 - accuracy: 0.8867\n",
      "Epoch 10/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3882 - accuracy: 0.9200\n",
      "Epoch 11/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3595 - accuracy: 0.9333\n",
      "Epoch 12/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3433 - accuracy: 0.9600\n",
      "Epoch 13/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3186 - accuracy: 0.9467\n",
      "Epoch 14/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2999 - accuracy: 0.9667\n",
      "Epoch 15/100\n",
      "150/150 [==============================] - 0s 991us/step - loss: 0.2806 - accuracy: 0.9733\n",
      "Epoch 16/100\n",
      "150/150 [==============================] - 0s 877us/step - loss: 0.2619 - accuracy: 0.9533\n",
      "Epoch 17/100\n",
      "150/150 [==============================] - 0s 783us/step - loss: 0.2515 - accuracy: 0.9533\n",
      "Epoch 18/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2362 - accuracy: 0.9800\n",
      "Epoch 19/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2273 - accuracy: 0.9667\n",
      "Epoch 20/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2131 - accuracy: 0.9600\n",
      "Epoch 21/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2038 - accuracy: 0.9600\n",
      "Epoch 22/100\n",
      "150/150 [==============================] - 0s 957us/step - loss: 0.1948 - accuracy: 0.9600\n",
      "Epoch 23/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1852 - accuracy: 0.9600\n",
      "Epoch 24/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1793 - accuracy: 0.9667\n",
      "Epoch 25/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1711 - accuracy: 0.9667\n",
      "Epoch 26/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1660 - accuracy: 0.9667\n",
      "Epoch 27/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1615 - accuracy: 0.9667\n",
      "Epoch 28/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1532 - accuracy: 0.9800\n",
      "Epoch 29/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1521 - accuracy: 0.9667\n",
      "Epoch 30/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1428 - accuracy: 0.9667\n",
      "Epoch 31/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1400 - accuracy: 0.9733\n",
      "Epoch 32/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1317 - accuracy: 0.9800\n",
      "Epoch 33/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9467\n",
      "Epoch 34/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9667\n",
      "Epoch 35/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1244 - accuracy: 0.9733\n",
      "Epoch 36/100\n",
      "150/150 [==============================] - 0s 997us/step - loss: 0.1219 - accuracy: 0.9667\n",
      "Epoch 37/100\n",
      "150/150 [==============================] - 0s 944us/step - loss: 0.1223 - accuracy: 0.9667\n",
      "Epoch 38/100\n",
      "150/150 [==============================] - 0s 810us/step - loss: 0.1202 - accuracy: 0.9533\n",
      "Epoch 39/100\n",
      "150/150 [==============================] - 0s 910us/step - loss: 0.1178 - accuracy: 0.9733\n",
      "Epoch 40/100\n",
      "150/150 [==============================] - 0s 997us/step - loss: 0.1118 - accuracy: 0.9733\n",
      "Epoch 41/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1092 - accuracy: 0.9600\n",
      "Epoch 42/100\n",
      "150/150 [==============================] - 0s 904us/step - loss: 0.1079 - accuracy: 0.9733\n",
      "Epoch 43/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.9667\n",
      "Epoch 44/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.9667\n",
      "Epoch 45/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1078 - accuracy: 0.9733\n",
      "Epoch 46/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.9667\n",
      "Epoch 47/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9667\n",
      "Epoch 48/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0996 - accuracy: 0.9733\n",
      "Epoch 49/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0960 - accuracy: 0.9667\n",
      "Epoch 50/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0953 - accuracy: 0.9667\n",
      "Epoch 51/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.9667\n",
      "Epoch 52/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.9600\n",
      "Epoch 53/100\n",
      "150/150 [==============================] - 0s 997us/step - loss: 0.0878 - accuracy: 0.9667\n",
      "Epoch 54/100\n",
      "150/150 [==============================] - 0s 917us/step - loss: 0.0950 - accuracy: 0.9667\n",
      "Epoch 55/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0901 - accuracy: 0.9733\n",
      "Epoch 56/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.9667\n",
      "Epoch 57/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.9667\n",
      "Epoch 58/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0884 - accuracy: 0.9600\n",
      "Epoch 59/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.9533\n",
      "Epoch 60/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.9733\n",
      "Epoch 61/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.9800\n",
      "Epoch 62/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9667\n",
      "Epoch 63/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.9733\n",
      "Epoch 64/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0820 - accuracy: 0.9667\n",
      "Epoch 65/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.9800\n",
      "Epoch 66/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0783 - accuracy: 0.9800\n",
      "Epoch 67/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 0.9733\n",
      "Epoch 68/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0806 - accuracy: 0.9733\n",
      "Epoch 69/100\n",
      "150/150 [==============================] - 0s 997us/step - loss: 0.0831 - accuracy: 0.9667\n",
      "Epoch 70/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.9800\n",
      "Epoch 71/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9733\n",
      "Epoch 72/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.9667\n",
      "Epoch 73/100\n",
      "150/150 [==============================] - 0s 944us/step - loss: 0.0740 - accuracy: 0.9733\n",
      "Epoch 74/100\n",
      "150/150 [==============================] - 0s 944us/step - loss: 0.0873 - accuracy: 0.9600\n",
      "Epoch 75/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0749 - accuracy: 0.9733\n",
      "Epoch 76/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.9667\n",
      "Epoch 77/100\n",
      "150/150 [==============================] - 0s 971us/step - loss: 0.0780 - accuracy: 0.9733\n",
      "Epoch 78/100\n",
      "150/150 [==============================] - 0s 870us/step - loss: 0.0772 - accuracy: 0.9667\n",
      "Epoch 79/100\n",
      "150/150 [==============================] - 0s 984us/step - loss: 0.0775 - accuracy: 0.9733\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.9733\n",
      "Epoch 81/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0719 - accuracy: 0.9667\n",
      "Epoch 82/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.9733\n",
      "Epoch 83/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.9800\n",
      "Epoch 84/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0749 - accuracy: 0.9733\n",
      "Epoch 85/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0802 - accuracy: 0.9533\n",
      "Epoch 86/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.9667\n",
      "Epoch 87/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0700 - accuracy: 0.9800\n",
      "Epoch 88/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0726 - accuracy: 0.9733\n",
      "Epoch 89/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.9800\n",
      "Epoch 90/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.9800\n",
      "Epoch 91/100\n",
      "150/150 [==============================] - 0s 977us/step - loss: 0.0727 - accuracy: 0.9733\n",
      "Epoch 92/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 0.9733\n",
      "Epoch 93/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0715 - accuracy: 0.9867\n",
      "Epoch 94/100\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0738 - accuracy: 0.9667\n",
      "Epoch 95/100\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0736 - accuracy: 0.9733\n",
      "Epoch 96/100\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0697 - accuracy: 0.9800\n",
      "Epoch 97/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0746 - accuracy: 0.9733\n",
      "Epoch 98/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.9733\n",
      "Epoch 99/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0679 - accuracy: 0.9800\n",
      "Epoch 100/100\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0670 - accuracy: 0.9733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2bbc2bc0f40>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=4, activation = 'relu'))\n",
    "model.add(Dense(3,activation = 'softmax'))\n",
    "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X,y, epochs=100, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b75e6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "150/150 [==============================] - 1s 1ms/step - loss: 1.2341 - accuracy: 0.2800\n",
      "Epoch 2/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 1.0637 - accuracy: 0.5733\n",
      "Epoch 3/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.9749 - accuracy: 0.6533\n",
      "Epoch 4/100\n",
      "150/150 [==============================] - 0s 870us/step - loss: 0.8952 - accuracy: 0.5933\n",
      "Epoch 5/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.8165 - accuracy: 0.6600\n",
      "Epoch 6/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.7443 - accuracy: 0.6600\n",
      "Epoch 7/100\n",
      "150/150 [==============================] - 0s 884us/step - loss: 0.6854 - accuracy: 0.6733\n",
      "Epoch 8/100\n",
      "150/150 [==============================] - 0s 877us/step - loss: 0.6316 - accuracy: 0.7333\n",
      "Epoch 9/100\n",
      "150/150 [==============================] - 0s 884us/step - loss: 0.5865 - accuracy: 0.8133\n",
      "Epoch 10/100\n",
      "150/150 [==============================] - 0s 843us/step - loss: 0.5457 - accuracy: 0.8333\n",
      "Epoch 11/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.5108 - accuracy: 0.7467\n",
      "Epoch 12/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4801 - accuracy: 0.9200\n",
      "Epoch 13/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4520 - accuracy: 0.9533\n",
      "Epoch 14/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4262 - accuracy: 0.9200\n",
      "Epoch 15/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4007 - accuracy: 0.9333\n",
      "Epoch 16/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3824 - accuracy: 0.9467\n",
      "Epoch 17/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3611 - accuracy: 0.9467\n",
      "Epoch 18/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3477 - accuracy: 0.9667\n",
      "Epoch 19/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3299 - accuracy: 0.9267\n",
      "Epoch 20/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3145 - accuracy: 0.9533\n",
      "Epoch 21/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2985 - accuracy: 0.9333\n",
      "Epoch 22/100\n",
      "150/150 [==============================] - 0s 810us/step - loss: 0.2876 - accuracy: 0.9533\n",
      "Epoch 23/100\n",
      "150/150 [==============================] - 0s 997us/step - loss: 0.2751 - accuracy: 0.9733\n",
      "Epoch 24/100\n",
      "150/150 [==============================] - 0s 917us/step - loss: 0.2640 - accuracy: 0.9667\n",
      "Epoch 25/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2546 - accuracy: 0.9600\n",
      "Epoch 26/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2444 - accuracy: 0.9800\n",
      "Epoch 27/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2346 - accuracy: 0.9733\n",
      "Epoch 28/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2258 - accuracy: 0.9667\n",
      "Epoch 29/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2177 - accuracy: 0.9600\n",
      "Epoch 30/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2094 - accuracy: 0.9733: 0s - loss: 0.2023 - accuracy: \n",
      "Epoch 31/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2041 - accuracy: 0.9800\n",
      "Epoch 32/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1953 - accuracy: 0.9733\n",
      "Epoch 33/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1918 - accuracy: 0.9667\n",
      "Epoch 34/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1832 - accuracy: 0.9600\n",
      "Epoch 35/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1790 - accuracy: 0.9800\n",
      "Epoch 36/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1737 - accuracy: 0.9667\n",
      "Epoch 37/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1683 - accuracy: 0.9733\n",
      "Epoch 38/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1652 - accuracy: 0.9800\n",
      "Epoch 39/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1615 - accuracy: 0.9733\n",
      "Epoch 40/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1536 - accuracy: 0.9667\n",
      "Epoch 41/100\n",
      "150/150 [==============================] - 0s 964us/step - loss: 0.1511 - accuracy: 0.9800\n",
      "Epoch 42/100\n",
      "150/150 [==============================] - 0s 924us/step - loss: 0.1465 - accuracy: 0.9800\n",
      "Epoch 43/100\n",
      "150/150 [==============================] - 0s 863us/step - loss: 0.1473 - accuracy: 0.9733\n",
      "Epoch 44/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1424 - accuracy: 0.9733\n",
      "Epoch 45/100\n",
      "150/150 [==============================] - 0s 776us/step - loss: 0.1387 - accuracy: 0.9733\n",
      "Epoch 46/100\n",
      "150/150 [==============================] - 0s 837us/step - loss: 0.1357 - accuracy: 0.9533\n",
      "Epoch 47/100\n",
      "150/150 [==============================] - 0s 857us/step - loss: 0.1343 - accuracy: 0.9667\n",
      "Epoch 48/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9733\n",
      "Epoch 49/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.9733\n",
      "Epoch 50/100\n",
      "150/150 [==============================] - 0s 997us/step - loss: 0.1234 - accuracy: 0.9733\n",
      "Epoch 51/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.9667\n",
      "Epoch 52/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9800\n",
      "Epoch 53/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.9733\n",
      "Epoch 54/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1187 - accuracy: 0.9733\n",
      "Epoch 55/100\n",
      "150/150 [==============================] - 0s 991us/step - loss: 0.1103 - accuracy: 0.9800\n",
      "Epoch 56/100\n",
      "150/150 [==============================] - 0s 971us/step - loss: 0.1153 - accuracy: 0.9733\n",
      "Epoch 57/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1130 - accuracy: 0.9800\n",
      "Epoch 58/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1092 - accuracy: 0.9667\n",
      "Epoch 59/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1112 - accuracy: 0.9667\n",
      "Epoch 60/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9733\n",
      "Epoch 61/100\n",
      "150/150 [==============================] - 0s 890us/step - loss: 0.1091 - accuracy: 0.9733\n",
      "Epoch 62/100\n",
      "150/150 [==============================] - 0s 863us/step - loss: 0.1057 - accuracy: 0.9733\n",
      "Epoch 63/100\n",
      "150/150 [==============================] - 0s 884us/step - loss: 0.0948 - accuracy: 0.9800\n",
      "Epoch 64/100\n",
      "150/150 [==============================] - 0s 890us/step - loss: 0.1034 - accuracy: 0.9733\n",
      "Epoch 65/100\n",
      "150/150 [==============================] - 0s 964us/step - loss: 0.1026 - accuracy: 0.9667\n",
      "Epoch 66/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9733\n",
      "Epoch 67/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1001 - accuracy: 0.9867\n",
      "Epoch 68/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0902 - accuracy: 0.9867\n",
      "Epoch 69/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1002 - accuracy: 0.9667\n",
      "Epoch 70/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9667\n",
      "Epoch 71/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.9667\n",
      "Epoch 72/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.9733\n",
      "Epoch 73/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9800\n",
      "Epoch 74/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.9667\n",
      "Epoch 75/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0907 - accuracy: 0.9867\n",
      "Epoch 76/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9800\n",
      "Epoch 77/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.9667\n",
      "Epoch 78/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0888 - accuracy: 0.9867\n",
      "Epoch 79/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.9667\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0873 - accuracy: 0.9667\n",
      "Epoch 81/100\n",
      "150/150 [==============================] - 0s 984us/step - loss: 0.0856 - accuracy: 0.9733\n",
      "Epoch 82/100\n",
      "150/150 [==============================] - 0s 863us/step - loss: 0.0845 - accuracy: 0.9800\n",
      "Epoch 83/100\n",
      "150/150 [==============================] - 0s 950us/step - loss: 0.0842 - accuracy: 0.9733\n",
      "Epoch 84/100\n",
      "150/150 [==============================] - 0s 904us/step - loss: 0.0847 - accuracy: 0.9667\n",
      "Epoch 85/100\n",
      "150/150 [==============================] - 0s 870us/step - loss: 0.0829 - accuracy: 0.9800\n",
      "Epoch 86/100\n",
      "150/150 [==============================] - 0s 857us/step - loss: 0.0851 - accuracy: 0.9733\n",
      "Epoch 87/100\n",
      "150/150 [==============================] - 0s 944us/step - loss: 0.0824 - accuracy: 0.9733\n",
      "Epoch 88/100\n",
      "150/150 [==============================] - 0s 971us/step - loss: 0.0789 - accuracy: 0.9667\n",
      "Epoch 89/100\n",
      "150/150 [==============================] - 0s 877us/step - loss: 0.0820 - accuracy: 0.9867\n",
      "Epoch 90/100\n",
      "150/150 [==============================] - 0s 830us/step - loss: 0.0820 - accuracy: 0.9867\n",
      "Epoch 91/100\n",
      "150/150 [==============================] - 0s 957us/step - loss: 0.0794 - accuracy: 0.9800\n",
      "Epoch 92/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0831 - accuracy: 0.9733\n",
      "Epoch 93/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.9800\n",
      "Epoch 94/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0824 - accuracy: 0.9733\n",
      "Epoch 95/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0774 - accuracy: 0.9867\n",
      "Epoch 96/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0803 - accuracy: 0.9733\n",
      "Epoch 97/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.9733\n",
      "Epoch 98/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.9733\n",
      "Epoch 99/100\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0788 - accuracy: 0.9867\n",
      "Epoch 100/100\n",
      "150/150 [==============================] - 0s 971us/step - loss: 0.0727 - accuracy: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2bbc40036d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=4, activation = 'relu'))\n",
    "model.add(Dense(3,activation = 'softmax'))\n",
    "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X,y, epochs=100, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a930a030",
   "metadata": {},
   "source": [
    "## CNN(Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "212554a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, InputLayer,MaxPool2D\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0420b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c666f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e74f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_test = np.expand_dims(X_test, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dff2b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[0])\n",
    "print(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32889488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d445c2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차원을늘림\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c75affe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c9dc73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 6)         60        \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4056)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                40570     \n",
      "=================================================================\n",
      "Total params: 40,630\n",
      "Trainable params: 40,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# 영상에 맞춰서 input을 구성 \n",
    "model.add(Input(shape=(28,28,1)))\n",
    "model.add(Conv2D(6, kernel_size = (3, 3), activation='relu'))\n",
    "#하나하나의 픽셀을 feature로 보고 일차원으로 나열 함\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdaaaa05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36ec4bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.4365 - accuracy: 0.8812 - val_loss: 0.2669 - val_accuracy: 0.9217\n",
      "Epoch 2/15\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.2383 - accuracy: 0.9323 - val_loss: 0.1947 - val_accuracy: 0.9486\n",
      "Epoch 3/15\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.1725 - accuracy: 0.9516 - val_loss: 0.1470 - val_accuracy: 0.9599\n",
      "Epoch 4/15\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.1345 - accuracy: 0.9626 - val_loss: 0.1267 - val_accuracy: 0.9652\n",
      "Epoch 5/15\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.1129 - accuracy: 0.9680 - val_loss: 0.1142 - val_accuracy: 0.9685\n",
      "Epoch 6/15\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.0961 - accuracy: 0.9731 - val_loss: 0.1046 - val_accuracy: 0.9700\n",
      "Epoch 7/15\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0849 - accuracy: 0.9765 - val_loss: 0.0987 - val_accuracy: 0.9712\n",
      "Epoch 8/15\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0775 - accuracy: 0.9779 - val_loss: 0.0950 - val_accuracy: 0.9722\n",
      "Epoch 9/15\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0702 - accuracy: 0.9802 - val_loss: 0.0913 - val_accuracy: 0.9732\n",
      "Epoch 10/15\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0643 - accuracy: 0.9817 - val_loss: 0.0872 - val_accuracy: 0.9758\n",
      "Epoch 11/15\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0574 - accuracy: 0.9837 - val_loss: 0.0842 - val_accuracy: 0.9753\n",
      "Epoch 12/15\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0530 - accuracy: 0.9845 - val_loss: 0.0854 - val_accuracy: 0.9753\n",
      "Epoch 13/15\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0480 - accuracy: 0.9864 - val_loss: 0.0889 - val_accuracy: 0.9739\n",
      "Epoch 14/15\n",
      "375/375 [==============================] - 5s 12ms/step - loss: 0.0438 - accuracy: 0.9875 - val_loss: 0.0896 - val_accuracy: 0.9741\n",
      "Epoch 15/15\n",
      "375/375 [==============================] - 4s 12ms/step - loss: 0.0414 - accuracy: 0.9878 - val_loss: 0.0891 - val_accuracy: 0.9738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2bbc96b3a00>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam',metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size = 128, epochs = 15, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2abf582f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.07837679237127304\n",
      "Test accuracy: 0.9751999974250793\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d1e66d",
   "metadata": {},
   "source": [
    "## Pooling Layer  \n",
    "- 출력 데이터의 크기를 줄이거나 특정 데이터를 강조하는 용도로 사용\n",
    "- Max Pooling과 Average Pooning, Min Pooling이 있음\n",
    "- 정사각 행렬의 특정 영역 안에 값의 최댓값을 모으거나 특정 영역의 평균을 구하는 방식으로 동작 \n",
    "- 일반적으로 Pooing 크기와 Stride를 같은 크기로 설정하여 모든 원소가 한 번씩 처리 되도록 설정  \n",
    "- CNN에서는 주로 Max Pooling을 사용  \n",
    "  \n",
    "    \n",
    "    \n",
    "- **Pooling 레이어, Convolution 레이어와 비교**\n",
    "  \n",
    "    - 학습대상 파라미터가 없음\n",
    "    - Pooling 레이어를 통과하면 행렬의 크기 감소\n",
    "    - Pooling 레이어를 통해서 채널 수 변경 없음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2425654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 26, 26, 6)         60        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 6)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1014)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                10150     \n",
      "=================================================================\n",
      "Total params: 10,210\n",
      "Trainable params: 10,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# 영상에 맞춰서 input을 구성 \n",
    "model.add(Input(shape=(28,28,1)))\n",
    "model.add(Conv2D(6, kernel_size = (3, 3), activation='relu'))\n",
    "# pool_size가 크면 파라미터는 줄어듬, 정보 손실 가능성 커짐 \n",
    "model.add(MaxPool2D(pool_size = (2,2)))\n",
    "#하나하나의 픽셀을 feature로 보고 일차원으로 나열 함\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e7b7d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.6150 - accuracy: 0.8356 - val_loss: 0.2892 - val_accuracy: 0.9171\n",
      "Epoch 2/15\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.2777 - accuracy: 0.9187 - val_loss: 0.2336 - val_accuracy: 0.9331\n",
      "Epoch 3/15\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.2290 - accuracy: 0.9347 - val_loss: 0.2009 - val_accuracy: 0.9466\n",
      "Epoch 4/15\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.1958 - accuracy: 0.9437 - val_loss: 0.1756 - val_accuracy: 0.9535\n",
      "Epoch 5/15\n",
      "375/375 [==============================] - 6s 15ms/step - loss: 0.1706 - accuracy: 0.9514 - val_loss: 0.1563 - val_accuracy: 0.9578\n",
      "Epoch 6/15\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.1519 - accuracy: 0.9564 - val_loss: 0.1408 - val_accuracy: 0.9622\n",
      "Epoch 7/15\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.1350 - accuracy: 0.9618 - val_loss: 0.1278 - val_accuracy: 0.9658\n",
      "Epoch 8/15\n",
      "375/375 [==============================] - 5s 14ms/step - loss: 0.1212 - accuracy: 0.9655 - val_loss: 0.1174 - val_accuracy: 0.9673\n",
      "Epoch 9/15\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.1084 - accuracy: 0.9699 - val_loss: 0.1062 - val_accuracy: 0.9708\n",
      "Epoch 10/15\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0988 - accuracy: 0.9731 - val_loss: 0.0996 - val_accuracy: 0.9722\n",
      "Epoch 11/15\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0906 - accuracy: 0.9749 - val_loss: 0.0939 - val_accuracy: 0.9743\n",
      "Epoch 12/15\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0841 - accuracy: 0.9763 - val_loss: 0.0909 - val_accuracy: 0.9746\n",
      "Epoch 13/15\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0786 - accuracy: 0.9783 - val_loss: 0.0854 - val_accuracy: 0.9764\n",
      "Epoch 14/15\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0740 - accuracy: 0.9789 - val_loss: 0.0839 - val_accuracy: 0.9756\n",
      "Epoch 15/15\n",
      "375/375 [==============================] - 5s 13ms/step - loss: 0.0701 - accuracy: 0.9800 - val_loss: 0.0802 - val_accuracy: 0.9772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2bbc987b610>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam',metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size = 128, epochs = 15, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445a1bc2",
   "metadata": {},
   "source": [
    "## 프로젝트 시 남들이 구현해 놓은 걸 테스트용도로 사용해 보는 것도 ㄱㅊ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c7ef7367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "171450368/171446536 [==============================] - 20s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet101\n",
    "conv_base = ResNet101(weights = 'imagenet', include_top= False, input_shape=(150,150,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b43de34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet101\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           [(None, 150, 150, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 156, 156, 3)  0           input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 75, 75, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 75, 75, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 75, 75, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 77, 77, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 38, 38, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 38, 38, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 38, 38, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 38, 38, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 38, 38, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 38, 38, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 38, 38, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 38, 38, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 38, 38, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 38, 38, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 38, 38, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 38, 38, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 38, 38, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 38, 38, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 38, 38, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 38, 38, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 38, 38, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 38, 38, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 38, 38, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 38, 38, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 38, 38, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 38, 38, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 38, 38, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 38, 38, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 38, 38, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 38, 38, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 38, 38, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 38, 38, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 38, 38, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 38, 38, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 38, 38, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 38, 38, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 38, 38, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 19, 19, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 19, 19, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 19, 19, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 19, 19, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 19, 19, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 19, 19, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 19, 19, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 19, 19, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 19, 19, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 19, 19, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 19, 19, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 19, 19, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 19, 19, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 19, 19, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 19, 19, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 19, 19, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 19, 19, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 19, 19, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 19, 19, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 19, 19, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 19, 19, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 19, 19, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 19, 19, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 19, 19, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 19, 19, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 19, 19, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 19, 19, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 19, 19, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 19, 19, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 19, 19, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 19, 19, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 19, 19, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 19, 19, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 19, 19, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 10, 10, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 10, 10, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 10, 10, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 10, 10, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 10, 10, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 10, 10, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 10, 10, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 10, 10, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 10, 10, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 10, 10, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 10, 10, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 10, 10, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 10, 10, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 10, 10, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 10, 10, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 10, 10, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 10, 10, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 10, 10, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 10, 10, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 10, 10, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 10, 10, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 10, 10, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 10, 10, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 10, 10, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 10, 10, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 10, 10, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 10, 10, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 10, 10, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 10, 10, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 10, 10, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 10, 10, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 10, 10, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 10, 10, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 10, 10, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 10, 10, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 10, 10, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 10, 10, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 10, 10, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 10, 10, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 10, 10, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 10, 10, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 10, 10, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 10, 10, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 10, 10, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, 10, 10, 256)  262400      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 10, 10, 256)  0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, 10, 10, 256)  590080      conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_relu (Activation (None, 10, 10, 256)  0           conv4_block7_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block7_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_3_bn (BatchNormali (None, 10, 10, 1024) 4096        conv4_block7_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_add (Add)          (None, 10, 10, 1024) 0           conv4_block6_out[0][0]           \n",
      "                                                                 conv4_block7_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_out (Activation)   (None, 10, 10, 1024) 0           conv4_block7_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, 10, 10, 256)  262400      conv4_block7_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 10, 10, 256)  0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, 10, 10, 256)  590080      conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_relu (Activation (None, 10, 10, 256)  0           conv4_block8_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block8_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_3_bn (BatchNormali (None, 10, 10, 1024) 4096        conv4_block8_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_add (Add)          (None, 10, 10, 1024) 0           conv4_block7_out[0][0]           \n",
      "                                                                 conv4_block8_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_out (Activation)   (None, 10, 10, 1024) 0           conv4_block8_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, 10, 10, 256)  262400      conv4_block8_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 10, 10, 256)  0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, 10, 10, 256)  590080      conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_relu (Activation (None, 10, 10, 256)  0           conv4_block9_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block9_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_3_bn (BatchNormali (None, 10, 10, 1024) 4096        conv4_block9_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_add (Add)          (None, 10, 10, 1024) 0           conv4_block8_out[0][0]           \n",
      "                                                                 conv4_block9_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_out (Activation)   (None, 10, 10, 1024) 0           conv4_block9_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, 10, 10, 256)  262400      conv4_block9_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 10, 10, 256)  0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, 10, 10, 256)  590080      conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_relu (Activatio (None, 10, 10, 256)  0           conv4_block10_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_3_conv (Conv2D)   (None, 10, 10, 1024) 263168      conv4_block10_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_3_bn (BatchNormal (None, 10, 10, 1024) 4096        conv4_block10_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_add (Add)         (None, 10, 10, 1024) 0           conv4_block9_out[0][0]           \n",
      "                                                                 conv4_block10_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_out (Activation)  (None, 10, 10, 1024) 0           conv4_block10_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, 10, 10, 256)  262400      conv4_block10_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 10, 10, 256)  0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, 10, 10, 256)  590080      conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_relu (Activatio (None, 10, 10, 256)  0           conv4_block11_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_3_conv (Conv2D)   (None, 10, 10, 1024) 263168      conv4_block11_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_3_bn (BatchNormal (None, 10, 10, 1024) 4096        conv4_block11_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_add (Add)         (None, 10, 10, 1024) 0           conv4_block10_out[0][0]          \n",
      "                                                                 conv4_block11_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_out (Activation)  (None, 10, 10, 1024) 0           conv4_block11_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, 10, 10, 256)  262400      conv4_block11_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 10, 10, 256)  0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, 10, 10, 256)  590080      conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_relu (Activatio (None, 10, 10, 256)  0           conv4_block12_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_3_conv (Conv2D)   (None, 10, 10, 1024) 263168      conv4_block12_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_3_bn (BatchNormal (None, 10, 10, 1024) 4096        conv4_block12_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_add (Add)         (None, 10, 10, 1024) 0           conv4_block11_out[0][0]          \n",
      "                                                                 conv4_block12_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_out (Activation)  (None, 10, 10, 1024) 0           conv4_block12_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, 10, 10, 256)  262400      conv4_block12_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 10, 10, 256)  0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, 10, 10, 256)  590080      conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_relu (Activatio (None, 10, 10, 256)  0           conv4_block13_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_3_conv (Conv2D)   (None, 10, 10, 1024) 263168      conv4_block13_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_3_bn (BatchNormal (None, 10, 10, 1024) 4096        conv4_block13_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_add (Add)         (None, 10, 10, 1024) 0           conv4_block12_out[0][0]          \n",
      "                                                                 conv4_block13_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_out (Activation)  (None, 10, 10, 1024) 0           conv4_block13_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, 10, 10, 256)  262400      conv4_block13_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 10, 10, 256)  0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, 10, 10, 256)  590080      conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_relu (Activatio (None, 10, 10, 256)  0           conv4_block14_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_3_conv (Conv2D)   (None, 10, 10, 1024) 263168      conv4_block14_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_3_bn (BatchNormal (None, 10, 10, 1024) 4096        conv4_block14_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_add (Add)         (None, 10, 10, 1024) 0           conv4_block13_out[0][0]          \n",
      "                                                                 conv4_block14_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_out (Activation)  (None, 10, 10, 1024) 0           conv4_block14_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, 10, 10, 256)  262400      conv4_block14_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 10, 10, 256)  0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, 10, 10, 256)  590080      conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_relu (Activatio (None, 10, 10, 256)  0           conv4_block15_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_3_conv (Conv2D)   (None, 10, 10, 1024) 263168      conv4_block15_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_3_bn (BatchNormal (None, 10, 10, 1024) 4096        conv4_block15_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_add (Add)         (None, 10, 10, 1024) 0           conv4_block14_out[0][0]          \n",
      "                                                                 conv4_block15_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_out (Activation)  (None, 10, 10, 1024) 0           conv4_block15_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, 10, 10, 256)  262400      conv4_block15_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 10, 10, 256)  0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, 10, 10, 256)  590080      conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_relu (Activatio (None, 10, 10, 256)  0           conv4_block16_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_3_conv (Conv2D)   (None, 10, 10, 1024) 263168      conv4_block16_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_3_bn (BatchNormal (None, 10, 10, 1024) 4096        conv4_block16_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_add (Add)         (None, 10, 10, 1024) 0           conv4_block15_out[0][0]          \n",
      "                                                                 conv4_block16_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_out (Activation)  (None, 10, 10, 1024) 0           conv4_block16_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)   (None, 10, 10, 256)  262400      conv4_block16_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, 10, 10, 256)  0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)   (None, 10, 10, 256)  590080      conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_relu (Activatio (None, 10, 10, 256)  0           conv4_block17_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_3_conv (Conv2D)   (None, 10, 10, 1024) 263168      conv4_block17_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_3_bn (BatchNormal (None, 10, 10, 1024) 4096        conv4_block17_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_add (Add)         (None, 10, 10, 1024) 0           conv4_block16_out[0][0]          \n",
      "                                                                 conv4_block17_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_out (Activation)  (None, 10, 10, 1024) 0           conv4_block17_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)   (None, 10, 10, 256)  262400      conv4_block17_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, 10, 10, 256)  0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)   (None, 10, 10, 256)  590080      conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_relu (Activatio (None, 10, 10, 256)  0           conv4_block18_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_3_conv (Conv2D)   (None, 10, 10, 1024) 263168      conv4_block18_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_3_bn (BatchNormal (None, 10, 10, 1024) 4096        conv4_block18_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_add (Add)         (None, 10, 10, 1024) 0           conv4_block17_out[0][0]          \n",
      "                                                                 conv4_block18_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_out (Activation)  (None, 10, 10, 1024) 0           conv4_block18_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)   (None, 10, 10, 256)  262400      conv4_block18_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, 10, 10, 256)  0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)   (None, 10, 10, 256)  590080      conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_relu (Activatio (None, 10, 10, 256)  0           conv4_block19_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_3_conv (Conv2D)   (None, 10, 10, 1024) 263168      conv4_block19_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_3_bn (BatchNormal (None, 10, 10, 1024) 4096        conv4_block19_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_add (Add)         (None, 10, 10, 1024) 0           conv4_block18_out[0][0]          \n",
      "                                                                 conv4_block19_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_out (Activation)  (None, 10, 10, 1024) 0           conv4_block19_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)   (None, 10, 10, 256)  262400      conv4_block19_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, 10, 10, 256)  0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)   (None, 10, 10, 256)  590080      conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_relu (Activatio (None, 10, 10, 256)  0           conv4_block20_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_3_conv (Conv2D)   (None, 10, 10, 1024) 263168      conv4_block20_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_3_bn (BatchNormal (None, 10, 10, 1024) 4096        conv4_block20_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_add (Add)         (None, 10, 10, 1024) 0           conv4_block19_out[0][0]          \n",
      "                                                                 conv4_block20_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_out (Activation)  (None, 10, 10, 1024) 0           conv4_block20_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)   (None, 10, 10, 256)  262400      conv4_block20_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, 10, 10, 256)  0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)   (None, 10, 10, 256)  590080      conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_relu (Activatio (None, 10, 10, 256)  0           conv4_block21_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_3_conv (Conv2D)   (None, 10, 10, 1024) 263168      conv4_block21_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_3_bn (BatchNormal (None, 10, 10, 1024) 4096        conv4_block21_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_add (Add)         (None, 10, 10, 1024) 0           conv4_block20_out[0][0]          \n",
      "                                                                 conv4_block21_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_out (Activation)  (None, 10, 10, 1024) 0           conv4_block21_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)   (None, 10, 10, 256)  262400      conv4_block21_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, 10, 10, 256)  0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)   (None, 10, 10, 256)  590080      conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_relu (Activatio (None, 10, 10, 256)  0           conv4_block22_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_3_conv (Conv2D)   (None, 10, 10, 1024) 263168      conv4_block22_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_3_bn (BatchNormal (None, 10, 10, 1024) 4096        conv4_block22_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_add (Add)         (None, 10, 10, 1024) 0           conv4_block21_out[0][0]          \n",
      "                                                                 conv4_block22_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_out (Activation)  (None, 10, 10, 1024) 0           conv4_block22_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)   (None, 10, 10, 256)  262400      conv4_block22_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, 10, 10, 256)  0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)   (None, 10, 10, 256)  590080      conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_bn (BatchNormal (None, 10, 10, 256)  1024        conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_relu (Activatio (None, 10, 10, 256)  0           conv4_block23_2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_3_conv (Conv2D)   (None, 10, 10, 1024) 263168      conv4_block23_2_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_3_bn (BatchNormal (None, 10, 10, 1024) 4096        conv4_block23_3_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_add (Add)         (None, 10, 10, 1024) 0           conv4_block22_out[0][0]          \n",
      "                                                                 conv4_block23_3_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_out (Activation)  (None, 10, 10, 1024) 0           conv4_block23_add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 5, 5, 512)    524800      conv4_block23_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 5, 5, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 5, 5, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 5, 5, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 5, 5, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 5, 5, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 5, 5, 2048)   2099200     conv4_block23_out[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 5, 5, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 5, 5, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 5, 5, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 5, 5, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 5, 5, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 5, 5, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 5, 5, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 5, 5, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 5, 5, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 5, 5, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 5, 5, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 5, 5, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 5, 5, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 5, 5, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 5, 5, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 5, 5, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 5, 5, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 5, 5, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 5, 5, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 5, 5, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 5, 5, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 5, 5, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 5, 5, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 5, 5, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 5, 5, 2048)   0           conv5_block3_add[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 42,658,176\n",
      "Trainable params: 42,552,832\n",
      "Non-trainable params: 105,344\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1eb9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "medel = Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c55cfbf",
   "metadata": {},
   "source": [
    "## RNN  \n",
    "- 반복적이고 순차적인 데이터(Sequential data)학습에 특화된 인공신경망의 한 종류\n",
    "- 음성 웨이브폼을 파악하거나, 텍스트의 앞 뒤 성분을 파악할 때 주로 사용\n",
    "- 시간에 종속된다는 특징\n",
    "- 현재 단어만 아니라 이전 단어에도 의존(<font color = 'red'>과거학습의 Weight를 통해 현재학습에 반영</font>)<p/>\n",
    "- RNN 단점은 처음시작한 Weight의 값이 점차 학습이 될 수록 상쇄 된다는 것 ---> 이를 해결한 알고리즘이 LSIM(Long Short Term Memory Network)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ee58f6f",
   "metadata": {},
   "source": [
    "### bidirectional  <p/>\n",
    "![image](https://user-images.githubusercontent.com/59672592/127452677-308696e5-3e92-491a-a7f1-ae949521010a.png)  \n",
    "양방향에서 weight를 받음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
