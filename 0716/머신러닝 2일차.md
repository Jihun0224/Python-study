## Feature Engineering - 데이터 전처리   
### 1.1 결측치 처리
```python 
DF.isna().sum()
DF.isnull().sum()
```
### 1.2 결측치 제외  
```python
# 누락된 값이 있는 행을 삭제
DF.dropna(axis=0)
# 누락된 값이 있는 열을 삭제
DF.dropna(axis=1)
# 모든 열이 NaN인 행을 삭제
DF.dropna(how='all')  
# NaN 아닌 값이 네 개보다 작은 행을 삭제
DF.dropna(thresh=4)
# 특정 열에 NaN이 있는 행만 삭제합니다(여기서는 'C'열)
DF.dropna(subset=['Age'])
```
### 1.3 대체값 삽입  
```python
# 행의 평균으로 누락된 값 대체하기
from sklearn.impute import SimpleImputer

imr = SimpleImputer(missing_values=np.nan, strategy='mean')
imr = imr.fit(DF[['Age']])
DF['Age_imp'] = imr.transform(DF[['Age']])
DF.head()
```
```python
DF['Age'].fillna(DF['Age'].mean())
```
- fillna() 메서드의 method 매개변수를 사용하여 누락된 값을 채울 수도 있음
- bfill 또는 backfill은 누락된 값을 다음 행의 값으로 채움. ffill 또는 pad는 누락된 값을 이전 행의 값으로 채웁니다.
```python
DF['Age'].fillna(method='bfill') # method='backfill'와 같습니다
DF['Age'].fillna(method='ffill') # method='pad'와 같습니다
```
## 2-범주형 데이터 다루기  
### 2.1 pandas map() 매핑  
```python
DF['Embarked'].isnull().sum()
mapping = {'C': 0,
           'Q': 1,
           'S': 2}
DF['Embarked_map'] = DF['Embarked'].map(mapping)
```
### 2.2 Class 레이블 인코딩  
```python 
from sklearn.preprocessing import LabelEncoder

# 사이킷런의 LabelEncoder을 사용한 레이블 인코딩
class_le = LabelEncoder()
y = class_le.fit_transform(DF['Embarked'].values)
# 거꾸로 매핑
class_le.inverse_transform(y)
```
### 2.3 One-Hot encoding  
```python
le = LabelEncoder()
le.fit_transform(DF['Pclass'])

```
```python
# pandas get_dummies( ) 메서드 사용
pd.get_dummies(titanic[['Sex', 'Embarked']])
```
![image](https://user-images.githubusercontent.com/59672592/125874824-8a3a7f20-9681-48e6-a1a4-a1c2dcde5021.png)
```python
# get_dummies에서 다중 공선성 문제 처리
pd.get_dummies(DF[['Sex', 'Embarked']], drop_first=True)
```
![image](https://user-images.githubusercontent.com/59672592/125874852-a0f7dc55-ce68-4dcf-9f9c-64b14000f366.png)

```python
from sklearn,preprocessing import OneHotEncoder
# OneHotEncoder에서 다중 공선성 문제 처리
ohe = OneHotEncoder(sparse=False, drop='first')
ohe_embarked = ohe.fit_transform(titanic[['Embarked']]).toarray()
```
---

## 3. 특성 스케일  
### 3.1 최소-최대 스케일링  

>$$x^{(i)}_{norm} = \dfrac{x^{(i)}-x_{min}}{x_{max}-x_{min}}$$

```python 
from sklearn.preprocessing import MinMaxScaler

mms = MinMaxScaler()
X_train_norm = mms.fit_transform(X_train)
X_test_norm = mms.transform(X_test)
```
### 3.2 표준화(standardization):
>$$x^{(i)}_{std} = \dfrac{x^{(i)}-\mu_x}{\sigma_x}$$
```python 
from sklearn.preprocessing import StandardScaler

stdsc = StandardScaler()
X_train_std = stdsc.fit_transform(X_train)
X_test_std = stdsc.transform(X_test)
```
### 3.3 표준화와 정규화 차이  
```python 
ex = np.array([0, 1, 2, 3, 4, 5])
# 표준화
print('표준화:', (ex - ex.mean()) / ex.std())
# 정규화
print('정규화:', (ex - ex.min()) / (ex.max() - ex.min()))
```
표준화: [-1.46385011 -0.87831007 -0.29277002  0.29277002  0.87831007  1.46385011]  
정규화: [0.  0.2 0.4 0.6 0.8 1. ]  
### 3.4 다양한 Scaler 함수 사용  
- `StandardScaler`, `MinMaxScaler`, `RobustScaler`, `MaxAbsScaler`에 대응하는 
- `scale()`, `minmax_scale()`, `robust_scale()`, `maxabs_scale()` 함수 사용  
### 3.5 Normalizer
- `Normalizer` 클래스와 `normalize()` 함수는 특성이 아니라 샘플별로 정규화를 수행
- 희소 행렬도 처리가능 
- 기본적으로 샘플의 L2 노름이 1이 되도록 정규화
```python
from sklearn.preprocessing import Normalizer

nrm = Normalizer()
X_train_l2 = nrm.fit_transform(X_train)
ex_2f = np.vstack((ex[1:], ex[1:]**2))
ex_2f
```
output = array([[ 1,  2,  3,  4,  5],
       [ 1,  4,  9, 16, 25]])  

---

### 범주형 데이터  
명목형 데이터 => 원핫엔코딩(주소에서 구,동으로 자름)
순서형 데이터 => 그룹화(아파트 층에 따라 지하/저층/고층 등으로 분리)  
순서형 데이터는 바로 써도됨. 순서형 데이터를 그룹화 해주고 싶으면 명목형으로 바꾸고 원핫엔코딩  

---

## Classification  
### 3 Types of Classification Predictions  
- Hard Prediction: 각 인스턴스에 대한 단일 범주를 예측
- Ranking Prediction: 가능성이 가장 높은 것부터 가장 낮은 것까지 인스턴스의
순위를 매김 (binary classification)
- Probability Prediction: 각 인스턴스에 클래스 전체의 확률 분포를 할당
## KNN(K Nearest Neighbors Classification)  
새로운 데이터를 입력받았을 때 가장 가까지 있는 것이 무엇이냐를 중심으로 새로운 데이터의 종류를 정해주는 알고리즘  
단순히 주변에 무엇이 가장 가까이 있는가를 보는 것이 아니라 주변에 있는 몇개의 것들을 같이 봐서 가장 많은 것을 골라내는 방식  
### KNN - 거리 측정 기법  
- 1) Manhattan distance  
![image](https://user-images.githubusercontent.com/59672592/125879368-d8009b3f-a515-431b-a58d-61c5515f602e.png)
- 2) Euclidean distance  
![image](https://user-images.githubusercontent.com/59672592/125880484-c477b4cf-67ed-41c8-99bf-8c15f4197ed2.png)
### KNN Model 특징  
- 단순히 데이터를 저장하기 때문에 모델 생성이 빠름
- 거리 계산이 많기 때문에 예측 속도가 느림
- 데이터 세트가 큰 경우 많은 메모리 요구
### KNN 사용  
```python
from sklearn.neighbors import KNeighborsClassifier
KNN = KNeighborsClassifier(n_neighbors=3)
KNN = KNN.fit(X_train, y_train)
y_predict = KNN.predict(X_test)
```
회귀 모델은 KNeighborsRegressor로 수행  

---

### Confusion Matrix  
![image](https://user-images.githubusercontent.com/59672592/125881456-017be9e1-3df3-45fd-8d00-754ac1da9e14.png)
- True Positives : 1인 레이블을 1이라 하는 경우를 True Positives라고 한다. -> 관심 범주를 정확하게 분류한 값.  
- False Negatives : 1인 레이블을 0이라 하는 경우를 False Negatives라고 한다. -> 관심 범주가 아닌것으로 잘못 분류함.  
- False Positives : 0인 레이블을 1이라 하는 경우를 False Positives라고 한다. -> 관심 범주라고 잘못 분류함.  
- True Negatives : 0인 레이블을 0이라 하는 경우를 True Negatives라고 한다. -> 관심 범주가 아닌것을 정확하게 분류.  
- 정확도는 1을 1로, 0을 0으로 정확하게 분류해낸 것을 의미한다. 모델이 얼마나 정확한지를 평가하는 척도  
- 정밀도는 모델을 통해 1이라고 분류해낸 그룹 A가 있을 때, 모델이 얼마나 믿을만한 정도로 A를 만들어 냈는지를 평가  
- 재현도는 정밀도와 비교되는 척도로써, 전체 예측 중에 TP가 얼마나 많은 가에 관한 것  
- F1-Score 는 Precision과 Recall의 가중치 조화 평균으로 Precison과 Recall을
동시에 확인할 때 측정지표로 활용  
```python 
from sklearn import metrics

accuracy = metrics.accuracy_score(y_test, y_pred)
cm = metrics.confusion_matrix(y_test, y_pred)
print(accuracy)
print(cm)
```
output: 0.9473684210526315  
[[16  0  0]  
 [ 0  7  1]  
 [ 0  1 13]]  

---

### ROC Curve와 AUC  
모델의 효율성을 평가하는 척도로 ROC Curve라는 것이 존재  
모델의 효율성을 민감도, 특이도를 이용하여 그래프  
ROC 커브와 x축이 이루고 있는 면적의 넓이를 AUC(Area Under Curve)라고 하는데, AUC의 값이 1에 가까울수록 효율적인 모델  
- x축은 FPR(False Positive Rate)로, 틀린것을 맞았다고 잘못 예측한 수치를 나타낸다. (FPR = FP / (FP + TN))  
- y축은 TPR(True Positive Rate)로, 맞은것을 맞았다고 잘 예측한 수치를 나타낸다. (TPR = TP / (TP + FN)) [== Recall]  
![image](https://user-images.githubusercontent.com/59672592/125882215-c02eb124-0322-4daa-bc88-36c1c56e15cb.png)

<strong>사용</strong>  
```python
from sklearn.metrics import accuracy_score
accuracy_value = accuracy_score(y_test, y_pred)
```
---

## Naïve Bayes  
### 베이즈 정리(Bayes Theorem)
![image](https://user-images.githubusercontent.com/59672592/125890250-96d07dab-d10e-43a6-80e5-ff59023634a4.png)

- 장점  
그룹이 여러 개 있는 multi-class 분류에서 특히 쉽고 빠르게 예측 가능  
독립이라는 가정이 유효하다고 한다면, logistic regression과 같은 다른 방식에 비해 훨씬 결과가 좋고, 학습 데이터도 적게 필요  
수치형 데이터 보다는 범주형 데이터에 특히 효과적  
- 단점  
학습 데이터에는 없고 검사 데이터에서는 있는 범주에서는 확률이 0이 되어 정상적인 예측이 불가능 -> zero frequency  --해결법--> smoothing technique  
독립이라는 가정이 성립하지 않거나 약한 경우라면 결과에 에러가 발생할 수 있기 때문에, 나오는 결과를 너무 신뢰하면 안됨  
- 사용  
```python
from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()
gnb.fit(X_train, y_train)
y_pred_2 = gnb.predict(X_test)

accuracy = metrics.accuracy_score(y_test, y_pred_2)
cm = metrics.confusion_matrix(y_test, y_pred_2)
print(metrics.classification_report(y_test, y_pred_2))
```
---

## Cross Validation  
보통은 train set 으로 모델을 훈련, test set으로 모델을 검증.  
고정된 test set을 통해 모델의 성능을 검증하고 수정하는 과정을 반복하면, 결국 만든 모델은 test set 에만 잘 동작하는 모델이 됨.  
즉, test set에 과적합(overfitting)하게 되므로, 다른 실제 데이터를 가져와 예측을 수행하면 엉망인 결과가 나와버리게 됨  
이를 해결하고자 하는 것이 바로 교차 검증(cross validation)이다.  
교차 검증은 train set을 train set + validation set으로 분리한 뒤, validation set을 사용해 검증하는 방식  

- 장점  
모든 데이터셋을 훈련에 활용 -> 정확도 향상, 데이터 부족으로 인한 underfitting 방지  
모든 데이터셋을 평가에 활용 -> 평가에 사용되는 데이터 편중을 막음, 좀 더 일반화된 모델 만들 수 있음  
- 단점   
모델 훈련/평가 시간이 오래 걸림  

- 기법 종류  
K-Fold Cross Validation ( k-겹 교차 검증 )  
Stratified K-Fold Cross Validation ( 계층별 k-겹 교차 검증 )  

```python
  from sklearn.model_selection import cross_val_score

  # 모델
  logreg = LogisticRegression()

  # (모델, Traingdata의 feature, Trainingdata의 target, 폴드수)
  scores = cross_val_score(logreg , X_train , y_train ,cv=3)

  # Trainingdata에 대한 성능을 나타낸다.
  print('교차 검증별 정확도:',np.round(scores, 4))
  print('평균 검증 정확도:', np.round(np.mean(scores), 4))
```
```python
  from sklearn.model_selection import KFold
  from sklearn.model_selection import cross_val_score

  # 모델
  logreg = LogisticRegression()

  # n_split : 몇개로 분할할지
  # shuffle : Fold를 나누기 전에 무작위로 섞을지
  # random_state : 나눈 Fold를 그대로 사용할지
  kfold = KFold(n_splits=6, shuffle = True, random_state=0)

  # (모델, Traingdata의 feature, Trainingdata의 target, 폴드수)
  scores = cross_val_score(logreg , X_train , y_train ,cv=kfold)

  # Trainingdata에 대한 성능을 나타낸다.
  print('교차 검증별 정확도:',np.round(scores, 4))
  print('평균 검증 정확도:', np.round(np.mean(scores), 4))
```
---

## Logistic Regression  
![image](https://user-images.githubusercontent.com/59672592/125942917-2c9b6bcd-040f-4fce-be96-84050549b78e.png)
사용자가 특정 기준을 정해서 분류

- 사용
```python
from sklearn.linear_model import LogisticRegression
# penalty = l1: 맨허튼, l2:유클리드 / c = 강도  
logreg = LR = LogisticRegression(penalty='l2', c=10.0)
logreg.fit(X_train, y_train)

y_pred_class = logreg.predict(X_test)
y_pred_prob = logreg.predict_proba(X_test)
# calculate classification accuracy
from sklearn import metrics

print(metrics.accuracy_score(y_test, y_pred_class))
```
---

## Hyperparameter Grid Search  
- 목표  
모델 하이퍼 파라미터에 넣을 수 있는 값들을 순차적으로 입력한 뒤에 가장 높은 성능을 보이는 하이퍼 파라미터들을 찾는 것  
- 하이퍼 파라미터란?  
+ 매개변수로 모델링에서 사용자가 직접 세팅해주는 값
+ 하이퍼파라미터는 최적의 값이 없으며,
+ 사용자의 휴리스틱이나 경험에 의해 결정 
ex) 랜덤 포레스트 모델을 만든다고 가정하면, 트리의 개수를 몇개까지 할 것인지. 트리의 깊이는 몇까지 할 것인지  
딥러닝 모델에서는 layer의 개수, 에폭(학습횟수)의 수 등  
반면, 파라미터는  
+ 매개변수로 모델링에서 모델 내부에서 결정되는 변수  
+ 파라미터는 데이터를 통해 구해지며,  
+ 사용자에 의해 조정되지 않는다.  
```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, 
                                    iris_data.target, test_size=0.2, 
                                    random_state=121)
dtree = DecisionTreeClassifier()

### parameter 들을 dictionary 형태로 설정
parameters = {'max_depth':[1,2,3], 'min_samples_split':[2,3]}

# param_grid의 하이퍼 파라미터들을 3개의 train, test set fold 로 나누어서 테스트 수행 설정.  
### refit=True 가 default 임. True이면 가장 좋은 파라미터 설정으로 재 학습 시킴.  
grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True)

# 붓꽃 Train 데이터로 param_grid의 하이퍼 파라미터들을 순차적으로 학습/평가 .
grid_dtree.fit(X_train, y_train)

# GridSearchCV 결과 추출하여 DataFrame으로 변환
scores_df = pd.DataFrame(grid_dtree.cv_results_)
scores_df[['params', 'mean_test_score', 'rank_test_score', \
           'split0_test_score', 'split1_test_score', 'split2_test_score']]
print('GridSearchCV 최적 파라미터:', grid_dtree.best_params_)
print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_dtree.best_score_))
# GridSearchCV의 refit으로 이미 학습이 된 estimator 반환
estimator = grid_dtree.best_estimator_

# GridSearchCV의 best_estimator_는 이미 최적 하이퍼 파라미터로 학습이 됨
pred = estimator.predict(X_test)
print('테스트 데이터 세트 정확도: {0:.4f}'.format(accuracy_score(y_test,pred)))
```
---

## Random Search  
- 개요  
탐색 대상 구간 내의 후보 hyperparameter 값들을 랜덤 샘플링(sampling)을 통해 선정  
- 장점  
Random Search는 Grid Search에 비해  
불필요한 반복 수행 횟수를 대폭 줄이면서,  
동시에 정해진 간격(grid) 사이에 위치한 값들에 대해서도 확률적으로 탐색이 가능하므로,  
최적 hyperparameter 값을 더 빨리 탐색  
- 방법  
모든 grid를 전부 search하는 대신 Random하게 일부의 parameter듦나 관측한 후, 그 중에서 가장 좋은 parameter를 선택하여 최적화  
- 특징  
Grid로 제한되지 않기 때문에 확률적으로 Important Parameter를 살펴보며 탐색  
- 단점  
이전까지의 조사 과정에서 얻어진 hyperparameter 값들의 성능 결과에 대한 '사전지식'이 전혀 미반영  

---
## Bayesian Optimization  
+ 필수 요소  
  + Surrogate Model - Gaussian Regression  
현재까지 조사된 입력값-함숫값 점들 (x1, f(x1)),...,(xt, f(xt))를 바탕으로 미지의 목적 함수의 형태에 대한 확률적인 추정을 수행하는 모델  
  + Acquisition Function - 
목적 함수에 대한 현재까지 확률적 추정 결과를 바탕으로, ‘최적 입력 값을 찾는 데 있어 가장 유용할만한’ 다음 입력 값 후보를 추천해 주는 함수  

1. 개요
어느 입력값(x)를 받는 미지의 목적 함수 f(x)를 상정하여 f(x)을 최대로 만들며 최적화의 해를 탐색
2. 방법론
• 목적 함수(탐색 대상함수)와 해당 하이퍼파라미터 쌍(pair)을 대상으로
• Surrogate Model(대체 모델) 을 만들고, 
• 평가를 통해 순차적으로 업데이트해 가면서 최적의 하이퍼파라미터 조합을 탐색
• 목적 함수를 black-box function라 명칭
3. 특징
매 회 새로운 hyperparameter 값에 대한 조사를 수행할 시 ‘사전 지식’을 충분히 반영하면서, 동시에
전체적인 탐색 과정을 체계적으로 수행
```python
pip install bayesian-optimization
from bayes_opt import BayesianOptimization
xgb_opt = BayesianOptimization(xgb_cv,
                                 {XGBoost_params
                                  }, random_state=1)
xgb_opt.maximize(init_points=2,n_iter=5)
best_params = xgb_opt.max['params']
```
---
