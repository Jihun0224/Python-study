# Regression: Regularization  
## Lasso Regression (L1 Norm)  
![image](https://user-images.githubusercontent.com/59672592/126089041-8b0cf309-e4bb-4e08-a895-af31b19d5e31.png)
- Manhattan norm 이라고도 함
- 백터의 요소에 대한 절댓값의 함
- 요소의 값 변화를 정확하게 파악 가능
- 패널티는 일부 계수를 선택적으로 축소효과 
- Feature Selection에 사용 가능
- Ridge Regression 보다 수렴 속도가 느림
---
## Ridge Regression (L2 Norm)  
![image](https://user-images.githubusercontent.com/59672592/126089023-b2197fb6-fbbf-493b-8de6-f987cce3275c.png)
- n차원 좌표평면(유클리드 공간)에서의 벡터의 크기를 계산하기 떄문에 유클리드 노름이라고도 함
-  Cost  = Loss + Penealty
- 패널티는 모든 계수의 크기를 축소
- 더 큰 계수는 제곱으로 강력하게 불이익으로 규제 효과
- x들 간의 분산을 줄이기 위해 beta 제곱에 대해 패널티 부여
---

## Elastic Net Regularization  
![image](https://user-images.githubusercontent.com/59672592/126089062-5961381a-aa40-4c9b-b608-269a1ab07a12.png)
- L1과 L2의 절충
- 변수도 줄이기 싶고, 분산도 줄이고 싶은 경우에 사용
- L1과 L2사이에 정규화 페널티를 분배하는 추가 매개변수 조정 요구

```python
from sklearn.linear_model import Lasso
LR = Lasso(alpha=1.0)
LR = LR.fit(X_train, y_train)
y_predict = LR.predict(X_test)

from sklearn.linear_model import Ridge
RR = Ridge(alpha=1.0)
RR = RR.fit(X_train, y_train)
y_predict = RR.predict(X_test)

from sklearn.linear_model import ElasticNet
EN = ElasticNet(alpha=1.0,l1_ratio=0.5)
EN = EN.fit(X_train, y_train)
y_predict = EN.predict(X_test)
```

# AI 관련 논문 사이트  
- [arxiv](https://arxiv.org)
    - 출판 전 여러 분야의 논문들을 수집해 놓은 사이트
- [arxiv-sanity](http://www.arxiv-sanity.com/)
    - arxiv에 지정된 기간 동안 원하는 항목에 대해 중요하고 있기 있는 논문들을 상단에 배치시켜 줌
- [paperswithcode](https://paperswithcode.com/)
    - arxiv-sanity보다 보기 수월하고 논문별 성능 비교와 코드가 어디에 위치해있는지 까지 보여주는 사이트
---
# Optimizaion Pipeline  
![image](https://user-images.githubusercontent.com/59672592/126092723-37bbdf93-dcf9-4c3b-a90b-6a2f08c7d0e6.png)
- GridSearch를 통해 모델 매개 변수를 최적화
- 이를 프로세스의 다른 단계 (예 : 특성 추출 및 변환)와 통합하는 것이 Pipeline의 핵심
- Scikit-Learn의 pipleline 통해 기능 변환 단계와 모델을 함께 연결
- 연속 단계는 데이터를 다음 단계로 보내기 전에 ‘적합’ 및 '변환'을 수행
```python
from sklearn.pipeline import Pipeline
# Create an instance of the class with estimators
estimators = [('scaler', MinMaxScaler()), ('lasso', Lasso())] 
Pipe = Pipeline(estimators)
# Fit the instance on the data and then predict the expected value
Pipe = Pipe.fit(X_train, y_train)
y_predict = Pipe.predict(X_test)
```
# Decision Tree  
## 결정 트리(Decision Tree)  
- 의사결정 규칙을 나무 구조로 나타내어 전체 자료를 몇 개의 작은 집단으로 나누어서 분석하는 기법
- 분류 및 회귀가 가능한 머신러닝 알고리즘
- 매우 복잡한 데이터셋도 학습할 수 있는 강력한 알고리즘
- 데이터의 스케일링이나 정규화 등의 사전 가공의 영향이 매우 적음
- 예측 성능을 향상시키기 위해 복잡한 규칙 구조를 가져야 하며, 이로 인한 과적합이 발생해 반대로 예측 성능이 저하되기도 함
- 이러한 단점이 앙상블 기법에서는 오히려 장점으로 작용
- 앙상블은 매우 많은 여러 개의 약한 학습기(예측 성능이 상대적으로 떨어지는 학습 알고리즘)을 결함해 확률적 보완과 오률가 발생한 부분에 대한 가중치를 계속 업데이트하면서 예측 성능을 향상시킴
- 결정 트리는 좋은 약한 학습기 역할을 함
- 최근에 많이 사용하는 랜덤 포레스트의 기본 구성 요소
## 엔트로피와 지니계수  
- 엔트로피
    - 정보 이득은 엔트로피라는 개념을 기반
- 지니계수
    - 경제학에서 불평등 지수를 나타낼 때 사용하는 계수(0이 가장 평등하고 1로 갈수록 불평등)

## 결정 트리 특징  
- 데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우
- ML 알고리즘 중 직관적으로 이해하기 쉬운 알고리즘
- maximum depth가 크면 세세하게 분류됨 -> Test data에 한해서만 좋은 성능이 나오고 다른 데이터에서는 성능이 떨어짐 -> overfitting


## 결정 트리 장단점  
- 장점
    - 정보의 '균일도'라는 룰을 기반으로 하고 있어서 쉽고 직관적
    - 피처의 스케일링이나 정규화 등의 사전 가공 영향도가 크지 않음
- 단점
    - 과적합으로 알고리즘 성능이 떨어진다. 이를 극복하기 위해 트리의 크기를 사전에 제한하는 튜닝 필요
## 결정 트리 분할  
![image](https://user-images.githubusercontent.com/59672592/126093678-c422fba2-2cd7-487f-a402-8bdf3a0e8706.png)
depth가 깊어질수록 예측 성능이 저하될 수 있으므로 max값을 정해줌  

```python
from sklearn.tree import DecisionTreeClassifier
# Create an instance of the class.
DTC = DecisionTreeClassifier(criterion='gini’,
                            max_features=10, max_depth=5)
# Fit the instance on the data and then predict the expected value.
DTC = DTC.fit(X_train, y_train)
y_predict = DTC.predict(X_test)
```
### classification parameter  
>sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, ccp_alpha=0.0)  

### Regression parameter  
>sklearn.tree.DecisionTreeRegressor(*, criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, ccp_alpha=0.0)  

# Ensemble 기법  
## 앙상블(Ensemble) 개요  
- 앙상블 방법
    - 서로 다른 or 같은 알고리즘을 결합함, 대부분은 동일한 알고리즘을 결합
    - 정형 데이터의 예측 영역에서 매우 높은 예측 성능
    - 유형: 보팅(Voting), 배깅(Bagging), 부스팅(Boosting), 스태킹(Stacking)
- 앙상블 종류
    - 배깅(Bagging)
        - 랜덤 포레스트(Random Forest): 뛰어난 예측 성능, 상대적으로 빠른 수행 시간, 유연성.
    - 부스팅(Boosting)
        - 그래디언트 부스팅(Fradient Boosting): 뛰어난 예측 성능, 수행시간이 너무 오래 걸려 최적화 모델 튜닝이 어려움
        - XgBoosting(eXtra Gradient Boosting)와 LightBoosting: 기존 그래디언트 부스팅의 예측 성능을 발전시키면서 수행시간을 단축시킴, 정형 데이터의 분류 영역에서 가장 활용도가 높은 알고리즘
    - 보팅(Voting): 여러 개의 분류기가 투표를 통해 최종 예측 결과를 결정
        - Hard Voting: 다수의 Classifier간 다수결로 최종 class 결정
        - Soft Voting: 다수의 Classifier들의 확률 평균으로 결정
    ![image](https://user-images.githubusercontent.com/59672592/126104542-31515239-8271-458f-920c-19c01630036e.png)

    - 스태킹(Stacking): 여러 가지 다른 모델의 예측 결괏값을 다시 학습 데이터로 만들어서 다른 모델로 재 학습시켜 결과를 예측하는 방법
## GBM(Gradient Boost Machine)  
- GBM 개요  
    - 부스팅 알고리즘은 여러 개의 약한 학습기(week learner)를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 통해 오류를 가선해 나가면서 학습하는 방식
    - AdaBoost: 오류 데이터에 가중치를 부여하면서 부스팅을 수행
- Gradient Boost Machine
    - 가중치 업데이트를 경사 하강법(Gradient Descent)을 이용
    - CART 기반의 다른 알고리즘과 마찬가지로 분류는 물론, 회귀도 가능
    - 일반적으로 GBM이 랜덤 포레스트보다는 예측 성능이 뛰어남
    - 그러나, 수행 시간이 오래 걸리고, 하이퍼 파라미터 튜닝 노력도 더 필요
---
## Bagging  
- 샘플을 여러 번 뽑아(Bootstrap) 각 모델을 학습시켜 결과물을 집계(Aggregration)하는 방법
![image](https://user-images.githubusercontent.com/59672592/126106795-1f73fefe-ace8-4bcb-8a1e-b40780fa4c32.png)
- 데이터로부터 부트스트랩 -> 부트스트랩한 데이터로 모델을 학습 -> 학습된 모델의 결과를 집계하여 최종 결과 값 도출  
- Categorical Data는 투표 방식(Votinig)으로 결과를 집계(전체 모델에서 예측한 값 중 가장 많은 값을 최종 예측값으로 선정)하며, Continuous Data는 평균으로 집계(각각의 결정 트리 모델이 예측한 값에 평균을 취해 최종 Bagging Model의 예측값을 결정)
![image](https://user-images.githubusercontent.com/59672592/126107278-0a0e5992-8b15-4fe7-a34c-1cc17ec35f47.png)
- Tree가 많을수록 Bagging 성능 향상
- 옆 차트에선 최대 개선은 일반적으로 약 50 Trees에서 최대 성능 기대

### Bagging: 에러 계산  
- Bootstrap된 샘플은각 트리에 대한 기본 제공 오류 추정치를 제공
- 데이터 하위 집합을 기반으로 트리생성
- 미사용 샘플에서 해당 트리의 오류측정
### Bagging 강점  
- 해석 및 구현 용이
- 이기종 입력 데이터 혀용
- 의사 결정 트리보다 변동성이 적음
- 병렬로 나무를 키울 수 있음
```python
from sklearn.ensemble import BaggingClassifier
# Create an instance of the class.
BC = BaggingClassifier(n_estimators=50)
# Fit the instance on the data and then predict the expected value.
BC = BC.fit(X_train, y_train) 
y_predict = BC.predict(X_test)
```
- Random Forest가 있기 때문에 잘 쓰지는 않음
---
# Random Foreset  
- Boosting이 나오기 전까지 많이 사용됨
- Tree들을 많이 만들기 때문에 연산 시간이 오래 걸림

![image](https://user-images.githubusercontent.com/59672592/126107383-fdb0b322-1a49-4492-9568-71e7b472029e.png)
- Bagging에 비해 Random Forest의 오류가 더욱 감소
- 오류가 해결 될 때까지 충분한 Tree를 생성
### Classification  
> sklearn.ensemble.RandomForestClassifier(n_estimators=’warn’, criterion=’gini’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)

``` python
from sklearn.ensemble import RandomForestClassifier
#Create an instance of the class.
RC = RandomForestClassifier(n_estimators=100, max_features=10)
# the instance on the data and then predict the expected value.
RC = RC.fit(X_train, y_train) 
y_predict = RC.predict(X_test)
```
- 나무의 갯수, 각 나무의 변수 선택 수를 파라미터로 설정
- 각각의 파라미터 집합에 대해 Train set으로 모델을 생성한 후, Validation set으로 성능 평가 ==> 가장 성능이 좋은 파라미터와 모델 선택
- Test set으로 모델의 예측 성능 평가

---

# Boosting  
//추가 예정

# Stacking: Heterogeneous(이기종) 결합 분류기  
- 모든 종류의 모델을 결합하여 Stacking 모델 생성
- Bagging과 비슷하지만 결정 트리에 국한되지 않고 여러 모델 결합 가능
- 기본 학습자의 출력은 Features을 생성하고 데이터와 재결합
- 기본 학습자의 출력은 과반수 투표 또는 가중치를 통해 결합
- 메타 학습자 매개 변수가 사용되는 경우 hold-out data 요구
- 증가하는 모델 복잡성 주의
![image](https://user-images.githubusercontent.com/59672592/126111771-2377f68c-aa77-491f-befa-2b02b9cd373c.png)
```python
from sklearn.ensemble import VotingClassifier
#Create an instance of the class.
VC = VotingClassifier(estimator_list, voting='hard', 
                                    weights=estimator_weight_list)
#Fit the instance on the data and then predict the expected value.
VC = V.fit(X_train, y_train)
y_predict = VC.predict(X_test)
```

# XGBoost(eXtra Gradient Boost)  
## XGBoost 개요  
- 트리 기반의 앙상블 학습에서 가장 각광받고 있는 알고리즘 중 하나
- 압도적인 수치는 아니지만, 분류에 있어서는 일반적으로 다른 머신러닝보다 뛰어난 예측 성능 보유
- GBM에 기반하고 있지만, GBM의 단점인 느린 수행 시간 및 과적합 규제(Regularization) 부재 등의문제를 해결해서 매우 각광받고 있음
## XGBoost 종류  
- 파이썬 래퍼 XGBoost: 초기의 독자적인 XGBoost 프레임워크 기반의 XGBoost
- 사이킷런 래퍼 XGBoost: 사이킷런과 연동되는 모듈
## XGBoost 하이퍼 파라미터  
- booster
    - gbtree(tree based model) 또는 gblinear(linear model) 중 선택, Default = 'gbtree'
- silent
    - Default = 1, 출력 메시지를 나타내고 싶지 않을 경우 1로 설정
- nthread
    - CPU 실행 스레드 개수 조정
    - Default는 전체 다 사용하는 것
    - 멀티코어/스레드 CPU 시스템에서 일부CPU만 사용할 때 변경  
    
|파라미터 명(파이썬 래퍼)|파라미터명(사이킷런 래퍼)|설 명|
|-----|---|---|
|eta(0.3)|learning eta(0.3)|GBM의 learning rate와 같은 파라미터 범위: 0 ~ 1|
|num_boost_around(10)|n_estimators(100)|생성할 weak learner의 수|
|min_child_weight(1)|min_child_weight(1)|GBM의 min_samples_leaf와 유사,관측치에 대한 가중치 합의 최소를 말하지만 GBM에서는 관측치 수에 대한 최소를 의미, 과적합 조절 용도, 범위: 0 ~ ∞|
|gamma(0)|min_split_loss(0)|리프노드의 추가분할을 결정할 최소손실 감소값, 해당값보다 손실이 크게 감소할 때 분리, 값이 클수록 과적합 감소효과, 범위: 0 ~ ∞|
|max_depth(6)|max_depth(3)|트리 기반 알고리즘의 max_depth와 동일, 0을 지정하면 깊이의 제한이 없음, 너무 크면 과적합(통상 3~10정도 적용), 범위: 0 ~ ∞|
|ambda(1)|reg_lambda(1)|L2 Regularization 적용 값, 피처 개수가 많을 때 적용을 검토, 클수록 과적합 감소 효과|
|alpha(0)|reg_alpha(0)|L1 Regularization 적용 값, 피처 개수가 많을 때 적용을 검토, 클수록 과적합 감소 효과|
## Overfitting 방지 Know-how  
- eta 값을 낮춥니다.(0.01 ~ 0.1) → eta 값을 낮추면
num_boost_round(n_estimator)를 반대로 높여준다
- max_depth 값을 낮춘다
- min_child_weight 값을 높인다
- gamma 값을 높인다
- subsample과 colsample_bytree를 낮춘다
## Early Stopping 기능  
- GBM의 경우 n_estimators에 지정된 횟수만큼 학습을 끝까지 수행하지만, XGB의 경우 오류가 더 이상
개선되지 않으면 수행을 중지
- n_estimators 를 200으로 설정하고, 조기 중단 파라미터 값을 50으로 설정하면, 1부터 200회까지 부스팅을
반복하다가
- 50회를 반복하는 동안 학습오류가 감소하지 않으면 더 이상 부스팅을 진행하지 않고 종료
- 예를 들어 100회에서 학습오류 값이 0.8인데 101~150회 반복하는 동안 예측 오류가 0.8보다 작은 값이 하나도 없으면 부스팅을 종료