# unSupervised Learning  
- Data points have nukown outcome  
## Types of unSupervised Learning  
- clustering
    - 데이터에서 알려지지 않은 구조 식별
- Dimensionality reduction
    - 구조적 특성을 사용하여 데이터 단순화  
## 비지도 학습 구분  
![image](https://user-images.githubusercontent.com/59672592/126261467-ab7044e9-0f80-430e-a12b-b3178cb26d89.png)
 
 ---
 ## K-Means Clustering  
 - 대표적인 분리형 군집화 알고리즘 가운데 하나
 - 각 군집은 하나의 중심(centroid)을 가짐
 - 각 개체는 가장 가까운 중심에 할당되며, 같은 중심에 할당된 개체들이 모여 하나의 군집을 형성
 - 사용자가 사전에 군집 수(k)가 정해야 알고리즘을 실행할 수 있음
 - k = 하이퍼파라메터(hyperparameter)
 - Inertia (관성): 각 점에서 해당 군집까지의 거리 제곱의 합
 - Inertia 값이 작을수록 더 단단한 클러스터에 해당
 ### 동작  
1. 일단 K개의 임의의 중심점(centroid)을 배치하고 
1. 각 데이터들을 가장 가까운 중심점으로 할당한다. (일종의 군집을 형성한다.)
1. 군집으로 지정된 데이터들을 기반으로 해당 군집의 중심점을 업데이트한다.
1. 2번, 3번 단계를 그래서 수렴이 될 때까지, 즉 더이상 중심점이 업데이트 되지 않을 때까지 반복한다.
### 예시  
- K를 문제 정의에 따라 선택(Questionn about n=k?)
- 4 개의 CPU 코어에서 유사한 작업 클러스터링 (K = 4)
- 대부분의 사람들을 덮을 수있는 10가지 크기의 의류 디자인 (K = 10)
- 20개 분야의 과학 논문 검색을 위한 탐색 인터페이스 (K = 20)
### 사용  
```python 
from sklearn.cluster import KMeans
# n_clusters: final number of clusters, k-means++: cluster initiation
kmeans = KMeans(n_clusters=k, init='k-means++')
kmeans.fit(data)
kmeans.predict(samples)
```
---

## Distance Metrics  
- 벡터간의 물리적 길이를 기반으로 측정하는 방법
- 길이에 가중치를 주는 방법에 따라 여러가지 지표로 나타 낼 수 있고, 이 방법에 의해 underfitting, overfitting을 야기시킬 수도 있음
1. Manhattan distance (L1) : stepped path  
![image](https://user-images.githubusercontent.com/59672592/126262691-3e29b62a-1067-4da7-a1ed-5c268c73567d.png)
2. Cosine distance  
![image](https://user-images.githubusercontent.com/59672592/126262728-c4fe8138-0a1c-4140-903f-49771496cd3d.png)
3. Euclidean distance (L2) : straight line  
![image](https://user-images.githubusercontent.com/59672592/126262655-57a08e96-8465-4aff-a36e-b7bb7ef65e0c.png)
### Euclidean과 Cosine Distance 비교  
- Euclidean은 좌표 기반 측정에 유용
- 코사인은 발생 위치가 덜 중요한 텍스트와 같은 데이터에 더 적합
- 유클리드 거리는 차원의 저주에 더 민감 

```python
from sklearn.metrics import pairwise_distances

dist = pairwise_distances(X,Y,
                        metric='euclidean')
# or
from sklearn.metrics import euclidean_distances
dist = euclidean_distances(X,Y)

```
---

## Dimensional Reduction  
### PCA (Principal component analysis)  
- 데이터에 내재된 분산을 잘 설명하는 주성분 (principal component: PC) 들을 찾아 내는 방법
- PCA는 데이터 하나 하나에 대한 성분을 분석하는 것이 아니라, 여러 데이터들이 모여 하나의 분포를 이룰 때 이 분포의 주 성분을 분석해 주는 방법 
- 여기서 주성분이라 함은 그 방향으로 데이터들의 분산이 가장 큰 방향벡터를 의미
- 주성분의수=변수의수
- 활용 방법
    - 분산을 많이 설명하는 상위 주성분을 선택하고, 이들을 새로운 변수로 사용*
    - 시각화
### Curse of Dimensionality: 차원의 저주  
- 이론적으로 기능을 늘리면 성능이 향상
- 실제로 기능이 많을수록 성능이 저하
- 필요한 훈련 예제의 수는 차원에 따라 기하급수적으로 증가
- 여기서 차원은 Features

#### PCA: The Syntax  
```python 
#Import the class containing the dimensionality reduction method.
from sklearn.decomposition import PCA
#Create an instance of the class.
#final number of dimensions
# whiten = scale and center data
PCAinst = PCA(n_components=3, whiten=True)
#Fit the instance on the data and then transform the data.
X_trans = PCAinst.fit_transform(X_train)
```
#### Truncated SVD: The Syntax  
```python 
#Import the class containing the dimensionality reduction method.
from sklearn.decomposition import TruncatedSVD
#Create an instance of the class.
#does not center data
SVD = TruncatedSVD(n_components=3)
#Fit the instance on the data and then transform the data.
X_trans = SVD.fit_transform(X_sparse)
```

#### Kernel PCA: The Syntax  
```python 
#Import the class containing the dimensionality reduction method.
from sklearn.decomposition import KernalPCA
#Create an instance of the class.
kPCA = KernalPCA(n_components=3, kernel='rbf',gamma=1.0)
#Fit the instance on the data and then transform the data.
X_trans = kPCA.fit_transform(X_train)
```
---

## Collaborative Filtering: 협업 필터링  
### 협업 필터링  
- 고객이 방문한 페이지, 구매한 제품, 다른 항목에 부여한 평점과 같은 사용자 행동 기반 모델링  
- 사용자 또는 제품간의 가장 유사한 항목 또는 콘텐츠를 사용자에게 추천
    - 사용자 기반은 User-based Collaborative Filtering
    - 아이템 기반은 Item-based Collaborative Filtering
    - 컨텐츠 기반은 Contents-based Collaborative Filtering
- 기본 가정은 과거에 유사한 콘텐츠, 상품을 보거나 구매 한 적이있는 사람들이 향후 유사한 종류의
콘텐츠 또는 상품을 보거나 구매할 가능성이 있다
- 한 사람이 항목 A, B, C를 구매하고 다른 사람이 과거에 항목 A, B, D를 구매했다면,
- 첫 번째 사람은 항목 D를 구매할 가능성이 높고 다른 사람은 그들은 그들 사이에 많은 유사점을 공유하므로 항목 C를 구매 추천  
