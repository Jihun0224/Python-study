## 워드 임베딩(Word Embedding)  
- Word Embedding은 단어를 숫자로 바꾸는 기법 중 하나
- 특정 단어의 주변의 단어를 이용하여 유사도 즉 숫자로 단어를 대체
- ex) 개와 고양이는 한 문장의 주변에 등장하는 비율이 자전거보다 높으므로 더욱 유사 
--- 
## 차원 이해  
- 워드 임베딩은 하나의 단어를 여러 차원을 이용해 표현
- ex) 스마트폰, 충전기, 노트북을 차원에 표현
    - 1차원  
    ![image](https://user-images.githubusercontent.com/59672592/126854279-6f34ec72-01a0-46e1-9731-3e3c72c8724c.png)  
    - 2차원  
    ![image](https://user-images.githubusercontent.com/59672592/126854298-60f46556-38ce-4e15-bc3b-846e660e713b.png)  
    - 3차원  
    ![image](https://user-images.githubusercontent.com/59672592/126854310-3ce8b43e-50e8-430a-b77c-7fb4c1653572.png)  
- 워드 임베딩은 이와 같은 방식으로 대량의 문서를 학습하여 모든 단어의 유사도를 복수의 차원으로 기록
---
## 최적의 Embedding Dimension  
- 한 단어가 몇개의 컬럼으로 표현되어야 하느냐에 대해서 정답은 없음
- 보통 50 ~ 200차원을 많이 선택, 속도가 문제되지 않으면 300차원을 선택하기도 함
- 차원을 늘일수록 훈련시간이 늘어나며, 보통 200차원 이상에서는 훈련 시간은 커지나 성능은 크게 개선되지 않음
![image](https://user-images.githubusercontent.com/59672592/126854419-86c03510-d0db-4bc1-be6d-cc52592a4546.png)
---
## Word Embedding을 이용한 모델  
- Word Embedding 모델을 이용한다는 것은 첫 노드의 가중치를 단어가 가지고 있는 벡터로 된 의미값을 이용하겠다는 것  
- 입력층 어휘에 대한 One-Hot-Encoding 과정은 없음




## 원-핫 벡터와 임베딩 벡터의 차이  
|-|One-Hot Encoding|Word Embedding|
|------|---|---|
|차원|고차원(단어 집합의 크기)|저차원|
|다른 표현|희소 벡터의 일종|밀집 벡터의 일종|
|표현 방법|수동|훈련 데이터로부터 학습함|
|값의 타입|1과 0|실수|
---
## Word2Vec  
- Word2Vec은 구글에서 발표한 워드 임베딩 기법
- 가장 기본적인 모델
- CBOW와 Skip-gram이라는 두 개의 하위 모델이 있음
- CBOW는 주변에 있는 문맥 단어로 타깃 단어 하나를 예측하는 방법
- Skip-gram은 타깃 단어를 가지고 주변 문맥 단어를 예측
- Skip-gram의 임베딩 품질이 CBOW보다 좋은 경향이 있음
![image](https://user-images.githubusercontent.com/59672592/126854691-fd993f67-9a76-4d7f-824e-ac73822750e8.png)
---
# 순환신경망  
## RNN(Recurrnet Neural Network)  
- RNN은 시퀀스 데이터(Sequential data)를 다루기에 적절한 알고리즘
- 시퀀스 데이터는 특정 순서가 가정될 수 있는 데이터
- 내부에 있는 순환 구조에 의해 현재 정보에 이전 정보가 쌓이면서 정보 표현이 가능한 알고리즘으로, 데이터가 순환되기 때문에 정보가 끊임없이 갱신될 수 있는 구  
![image](https://user-images.githubusercontent.com/59672592/126858501-96d7b74c-f552-4e4a-8d2e-d3f9fe1231e8.png)
---
## 일반 순환신경망의 사용  
- 순환신경망은 input이 3D(batch_size, time_steps, feature_len)이기 때문에 보통 embedding 층과 연결하여 사용한다
- Embedding 층은 Flatten()하지 않으면 3D로 출력됨
- 임베딩의 결과로 RNN이 요구하는 3D 포맷이 만들어짐
![image](https://user-images.githubusercontent.com/59672592/126858508-1ccbd9d6-e1b4-4271-803a-27f319cb3837.png)  
---
## 다층 RNN  
- RNN 층은 복수로 쌓을 수 있다  
![image](https://user-images.githubusercontent.com/59672592/126858527-1a3cc0d3-7b17-4f67-8635-02976279a3de.png)  
---
## RNN의 문제점  
- RNN은 은닉층의 가중치가 지속적으로 곱해지는 것이므로  
가중치 절대값이 1보다 작으면 결국 매우 작은 값이 도출되고(그래디언트 소실),  
가중치 절대값이 1보다 크면 결국 매우 큰 값이 도출된다(그래디언트 폭주)
- 특히 그래디언트 소실이 일어난다.  
가중치가 활성화 함수 tanh를 지나며 매우 작아져 정보 전달이 안된다.
- 문장이 길어질수록 이 현상이 심해지며, 이를 앞의 정보가 뒤로 잘 전달되지 못한다라고 표현한다
---
## LSTM(Long Short-Term Memory)  
- RNN의 한계를 극복하기 위해 고안됨
- LSTM은 과거 정보의 소실이 일어나지 않도록 한다
- 이를 위해 셀 상태(장기 상태), 입력 게이트, 삭제 게이트, 출력 게이트의 네
가지 장치를 만들었다
- 네 가지 장치를 위한 연산이 각각 별도로 존재하기 때문에 RNN에 비하여
계산량이 많아 속도가 느린 점이 단점이다
- 그러나 긴 문장에서도 그래디언트 소실이 잘 일어나지 않는 것이 장점이
다. 즉, 정보의 소실이 많이 발생하지 않는다(오래된 과거의 정보를 더 잘 기억한다)
- 최근 GPU 등의 사용으로 연산속도가 빨라져 연산 시간에 대한 부담이 많
이 줄어들었다
## LSTM 구조  
![image](https://user-images.githubusercontent.com/59672592/126858788-fa4a29c5-2610-47f2-86c1-3be524567e5d.png)

**공부 예정**
---
## GRU(Gated Reccurent Unit)  
- GRU는 2014년 조경현의 논문에서 발표
- LSTM과 같이 다양한 게이트를 두지만 보다 단순화하여 계산량을 줄임
- GRU가 학습할 가중치가 적다는 것이 확실한 이점
- reset gate, update gate 2개의 gate만을 사용
- cell state, hidden state가 합쳐져 하나의 hidden state로 표현

